{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aabc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from ipyannotator.annotator import Annotator\n",
    "from ipyannotator.base import Settings\n",
    "from ipyannotator.mltypes import InputImage, OutputVideoBbox, NoOutput\n",
    "from ipyannotator.datasets.factory import DS, get_settings\n",
    "from ipyannotator.helpers import Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d61168",
   "metadata": {},
   "source": [
    "# Video Annotator - Tracking objects through video frames\n",
    "\n",
    "The current tutorial illustrates how to use Ipyannotator to classify video data.\n",
    "\n",
    "The task of identifying objects in a video frame is called video classification. \n",
    "\n",
    "**Ipyannotator** allows users to explore an entire set of video frames and specific labels; manually **create** their datasets drawing bounding boxes and associating labels across the frames; **improve** existing annotations.\n",
    "\n",
    "This tutorial is divided in the following steps:\n",
    "\n",
    "- [Select dataset](#Select-dataset)\n",
    "- [Setup annotator](#Setup-annotator)\n",
    "- [Explore](#Explore)\n",
    "- [Create](#Create)\n",
    "- [Improve](#Improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55531473",
   "metadata": {},
   "source": [
    "## Select dataset\n",
    "\n",
    "This tutorial uses a minimal artificial video dataset generated by Ipyannotator. The dataset follows [MOT data format](https://github.com/JonathonLuiten/TrackEval/blob/master/docs/MOTChallenge-Official/Readme.md#data-format). It contains 20 images with 2 classes (`rectangle` and  `circle`) and doesn't need to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DS.ARTIFICIAL_VIDEO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7d7e9",
   "metadata": {},
   "source": [
    "## Setup annotator\n",
    "\n",
    "\n",
    "This section will set up the paths and the input/output pair needed to classify the images.\n",
    "\n",
    "The following cell will import the project file and directory where the images were generated. For this tutorial we simplify the process using the `get_settings` function instead of hardcoding the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_ = get_settings(dataset)\n",
    "settings_.project_file, settings_.image_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822ce9",
   "metadata": {},
   "source": [
    "Ipyannotator uses pairs of input/output data to set up the annotation. \n",
    "\n",
    "The video image classification annotator uses `InputImage` and `OutputVideoBox`as the pair to set up the annotator.\n",
    "\n",
    "The `InputImage` function provides information about the directory that contains the images to be classified, and the images itself. The `OutputImageBox` function provides information about the directory that contains the classes that can be associated with the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = InputImage(image_dir=settings_.image_dir,\n",
    "                    image_width=settings_.im_width,\n",
    "                    image_height=settings_.im_height)\n",
    "\n",
    "output_ = OutputVideoBbox(classes=['Circle', 'Rectangle'])\n",
    "\n",
    "input_.dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4ee8",
   "metadata": {},
   "source": [
    "The final part in setting up the Ipyannotator is the configuration of the `Annotator` factory with the pair of input/output data. \n",
    "\n",
    "The factory allows three types of annotator tools: explore, create, improve. The next sections will guide you through every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "anni = Annotator(input_, NoOutput(), settings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a3401",
   "metadata": {},
   "source": [
    "## Explore\n",
    "The **explore** option allows users to navigate across the images in the dataset using `next/previous` buttons. This function is used for data visualization only, improvement and additional labeling is done in the next steps. \n",
    "\n",
    "When exploring the artificial dataset used in this tutorial you will see a red circle and a gray rectangle as the objects to be tracked. The black square represents an occlusion on the objects and is used to illustrate how the **improve** step works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = anni.explore()\n",
    "explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dfef7",
   "metadata": {},
   "source": [
    "## Create\n",
    "The **create** option allows users to manually create their annotated datasets. Please be aware that\n",
    "\n",
    "```{warning}\n",
    "The video annotator create option is a beta version\n",
    "```\n",
    "\n",
    "Currently, video annotation allows users to draw multiple bounding boxes in every frame and associate a label to every annotated object bounding box. Ipyannotator generates the objects creating indexed labels that start from 0.\n",
    "\n",
    "The next cell removes already created annotation files to create a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = 'data/artificial/create_results'\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b2f91",
   "metadata": {},
   "source": [
    "The next cell initializes the **create** option. \n",
    "\n",
    "For this tutorial, a function was defined that imitates human work, annotating the images automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "anni.output_item = output_\n",
    "creator = anni.create()\n",
    "creator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bd327-6a32-4793-bd0f-f3ab6e56022a",
   "metadata": {},
   "source": [
    "The next cell imitate human work annotating all images automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47881b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HELPER = Tutorial(dataset, settings_.project_path)\n",
    "annotations = HELPER.annotate_video_bboxes(creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fd83f",
   "metadata": {},
   "source": [
    "All data is stored in a file formatted as JSON in the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = {k: v for i, (k, v) in enumerate(annotations.items()) if i == 0}\n",
    "print(json.dumps(data_format, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2a176",
   "metadata": {},
   "source": [
    "Note that in the JSON file above the annotations of each frame is mapped by the path of the image. Every bounding box drawn in the annotators has the properties: `x`, `y`, `width`, `height`, `id`  as part of the `bbox` field. The annotation labels are mapped in the `labels` field in the JSON file. Every index of the `labels` array corresponds to the object mapped in the `bbox` property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d1610",
   "metadata": {},
   "source": [
    "## Improve\n",
    "\n",
    "The **improve** feature in the Ipyannotator video annotation allows users to refine the annotated dataset. This includes:\n",
    "\n",
    "- Select objects across the frames and join the trajectories drawn.\n",
    "- Update labels across the entire annotation.\n",
    "\n",
    "In the example below we have an occlusion illustrated by a black square. The rectangle disappears behind the occluding object and appears again with a new object id. The video annotator allows users to join the trajectories of different objects into a new object.  \n",
    "\n",
    "Joining trajectory:\n",
    "\n",
    "- Navigate across the annotator\n",
    "- Note that the gray rectangle disappears\n",
    "- Note that the gray rectangle reappears but with a new id\n",
    "- Select the rectangle with a new id (marking the checkbox)\n",
    "- Navigate back until you see the old gray rectangle id\n",
    "- Select the rectangle with the old id (marking the checkbox)\n",
    "- Click on the join button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ffbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "improver = anni.improve()\n",
    "improver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
